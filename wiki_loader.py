# -*- coding: utf-8 -*-
"""
í•œêµ­ ìœ„í‚¤í”¼ë””ì•„ CirrusSearch ë¤í”„ ë°ì´í„° ë¡œë”
- Wikipedia CirrusSearch JSON ë¤í”„ ë‹¤ìš´ë¡œë“œ
- Elasticsearch ìƒ‰ì¸
"""

import os
import gzip
import json
from typing import Generator, Dict
from elasticsearch import Elasticsearch, helpers


class WikipediaCirrusLoader:
    """í•œêµ­ ìœ„í‚¤í”¼ë””ì•„ CirrusSearch ë¤í”„ ë¡œë”"""

    def __init__(
        self,
        es_url: str = "http://localhost:9200",
        index_name: str = "kowiki_cirrus",
        username: str = "elastic",
        password: str = "hosun",
    ):
        """
        ì´ˆê¸°í™”

        Args:
            es_url: Elasticsearch URL
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
            username: Elasticsearch ì‚¬ìš©ìëª…
            password: Elasticsearch ë¹„ë°€ë²ˆí˜¸
        """
        self.es_url = es_url
        self.index_name = index_name
        self.client = Elasticsearch([es_url], basic_auth=(username, password))

    def check_connection(self):
        """Elasticsearch ì—°ê²° í™•ì¸"""
        if self.client.ping():
            print("âœ… Elasticsearch ì—°ê²° ì„±ê³µ!")
            info = self.client.info()
            print(f"   ë²„ì „: {info['version']['number']}")
            print(f"   í´ëŸ¬ìŠ¤í„°: {info['cluster_name']}")
            return True
        else:
            print("âŒ Elasticsearch ì—°ê²° ì‹¤íŒ¨!")
            return False

    def create_index(self):
        """ìœ„í‚¤í”¼ë””ì•„ìš© ì¸ë±ìŠ¤ ìƒì„±"""
        # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚­ì œ
        if self.client.indices.exists(index=self.index_name):
            self.client.indices.delete(index=self.index_name)
            print(f"ğŸ“ ê¸°ì¡´ ì¸ë±ìŠ¤ '{self.index_name}' ì‚­ì œë¨")

        # ì¸ë±ìŠ¤ ì„¤ì •
        index_settings = {
            "settings": {
                "analysis": {
                    "tokenizer": {
                        "nori_user_dict": {
                            "type": "nori_tokenizer",
                            "decompound_mode": "mixed",
                            "discard_punctuation": "true",
                        }
                    },
                    "analyzer": {
                        "korean_analyzer": {
                            "type": "custom",
                            "tokenizer": "nori_user_dict",
                            "filter": [
                                "lowercase",
                                "nori_part_of_speech",
                                "nori_readingform",
                            ],
                        }
                    },
                },
                "similarity": {"default": {"type": "BM25", "k1": 1.2, "b": 0.75}},
            },
            "mappings": {
                "properties": {
                    "title": {
                        "type": "text",
                        "analyzer": "korean_analyzer",
                        "fields": {"keyword": {"type": "keyword"}},
                    },
                    "text": {"type": "text", "analyzer": "korean_analyzer"},
                    "category": {"type": "keyword"},
                    "timestamp": {"type": "date"},
                    "template": {"type": "keyword"},
                    "namespace": {"type": "integer"},
                    "redirect": {"type": "keyword"},
                    "incoming_links": {"type": "integer"},
                    "opening_text": {"type": "text", "analyzer": "korean_analyzer"},
                }
            },
        }

        # ì¸ë±ìŠ¤ ìƒì„±
        self.client.indices.create(index=self.index_name, body=index_settings)
        print(f"âœ… ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì™„ë£Œ!")
        print("   - Nori Tokenizer ê¸°ë°˜ korean_analyzer ì ìš©")
        print("   - BM25 ìœ ì‚¬ë„ ì•Œê³ ë¦¬ì¦˜ ì„¤ì •")

    def parse_local_cirrus_file(
        self, file_path: str, max_docs: int = None
    ) -> Generator[Dict, None, None]:
        """
        ë¡œì»¬ì— ì €ì¥ëœ CirrusSearch ë¤í”„ íŒŒì¼ì„ íŒŒì‹±

        Args:
            file_path: ë¡œì»¬ .json.gz íŒŒì¼ ê²½ë¡œ
            max_docs: ìµœëŒ€ ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ì „ì²´)

        Yields:
            ë¬¸ì„œ ë”•ì…”ë„ˆë¦¬
        """

        print(f"\nğŸ“‚ ë¡œì»¬ íŒŒì¼ ì½ê¸° ì‹œì‘: {file_path}")

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(file_path)
        print(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_size / (1024**3):.2f} GB")

        doc_count = 0
        processed_lines = 0

        # gzipìœ¼ë¡œ ì••ì¶•ëœ íŒŒì¼ì„ í•œ ì¤„ì”© ì½ê¸°
        with gzip.open(file_path, "rt", encoding="utf-8") as gz_file:
            for line in gz_file:
                processed_lines += 1

                # CirrusSearch í˜•ì‹: ë‘ ì¤„ì”© ìŒì„ ì´ë£¸
                # ì²« ë²ˆì§¸ ì¤„: index action (ê±´ë„ˆë›°ê¸°)
                # ë‘ ë²ˆì§¸ ì¤„: ì‹¤ì œ ë¬¸ì„œ ë°ì´í„°
                if processed_lines % 2 == 0:
                    try:
                        doc = json.loads(line)
                        yield doc
                        doc_count += 1

                        if doc_count % 1000 == 0:
                            print(f"   ì²˜ë¦¬ ì¤‘... {doc_count:,}ê°œ ë¬¸ì„œ")

                        if max_docs and doc_count >= max_docs:
                            print(f"âš ï¸  ìµœëŒ€ ë¬¸ì„œ ìˆ˜({max_docs})ì— ë„ë‹¬í•˜ì—¬ ì¤‘ë‹¨")
                            break

                    except json.JSONDecodeError as e:
                        print(f"âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue

        print(f"âœ… ì´ {doc_count:,}ê°œ ë¬¸ì„œ íŒŒì‹± ì™„ë£Œ")

    def bulk_index_documents(
        self,
        local_file: str = None,
        max_docs: int = None,
        batch_size: int = 500,
    ):
        """
        ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œë¥¼ Elasticsearchì— bulk ìƒ‰ì¸

        Args:
            cirrus_url: CirrusSearch ë¤í”„ URL (ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ ì‹œ)
            local_file: ë¡œì»¬ .json.gz íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ íŒŒì¼ ì‚¬ìš© ì‹œ)
            max_docs: ìµœëŒ€ ìƒ‰ì¸í•  ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            batch_size: bulk ìƒ‰ì¸ ë°°ì¹˜ í¬ê¸°
        """
        if not local_file:
            raise ValueError(
                "cirrus_url ë˜ëŠ” local_file ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤."
            )

        print("\nğŸ“Š Elasticsearchì— ë¬¸ì„œ ìƒ‰ì¸ ì‹œì‘...")

        def generate_actions():
            """Elasticsearch bulk APIìš© action ìƒì„±"""
            # ë¡œì»¬ íŒŒì¼ ë˜ëŠ” URLì—ì„œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°

            doc_generator = self.parse_local_cirrus_file(local_file, max_docs)

            for idx, doc in enumerate(doc_generator):
                # ğŸ” ë””ë²„ê¹…: ì²« 3ê°œ ë¬¸ì„œì˜ ì›ë³¸ ë°ì´í„° êµ¬ì¡° ì¶œë ¥
                if idx < 3:
                    print(f"\n{'='*80}")
                    print(f"ğŸ“ ë¬¸ì„œ #{idx + 1} ì›ë³¸ ë°ì´í„° êµ¬ì¡°:")
                    print(f"{'='*80}")
                    print(f"ğŸ“Œ ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œ: {list(doc.keys())}")
                    print(
                        f"ğŸ“„ title: {doc.get('title', 'N/A')[:100] if doc.get('title') else 'N/A'}"
                    )
                    print(f"ğŸ“„ text ê¸¸ì´: {len(doc.get('text', ''))} ì")
                    if doc.get("text"):
                        print(f"ğŸ“„ text ë¯¸ë¦¬ë³´ê¸°: {doc.get('text', '')[:200]}...")
                    print(f"ğŸ“„ namespace: {doc.get('namespace', 'N/A')}")
                    print(f"ğŸ“„ timestamp: {doc.get('timestamp', 'N/A')}")
                    print(
                        f"ğŸ“„ opening_text: {doc.get('opening_text', 'N/A')[:150] if doc.get('opening_text') else 'N/A'}..."
                    )
                    print(f"{'='*80}\n")

                # ì¸ë±ì‹±í•  ë¬¸ì„œ êµ¬ì¡° ì •ë¦¬
                source = {
                    "title": doc.get("title", ""),
                    "text": doc.get("text", ""),
                    "timestamp": doc.get("timestamp"),
                    "namespace": doc.get("namespace"),
                }

                # ì„ íƒì  í•„ë“œ ì¶”ê°€
                if "category" in doc:
                    source["category"] = doc["category"]
                if "template" in doc:
                    source["template"] = doc["template"]
                if "redirect" in doc:
                    # redirectê°€ ê°ì²´ì¸ ê²½ìš° titleë§Œ ì¶”ì¶œ, ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    redirect_value = doc["redirect"]
                    if isinstance(redirect_value, dict):
                        source["redirect"] = redirect_value.get("title", "")
                    elif isinstance(redirect_value, str):
                        source["redirect"] = redirect_value
                if "incoming_links" in doc:
                    source["incoming_links"] = doc["incoming_links"]
                if "opening_text" in doc:
                    source["opening_text"] = doc["opening_text"]

                # ğŸ” ë””ë²„ê¹…: ì²« 3ê°œ ë¬¸ì„œì˜ ìƒ‰ì¸ë  ë°ì´í„° ì¶œë ¥
                if idx < 3:
                    print("âœ… ìƒ‰ì¸ë  ë°ì´í„°:")
                    for key, value in source.items():
                        if isinstance(value, str) and len(value) > 100:
                            print(f"   - {key}: {value[:100]}... (ê¸¸ì´: {len(value)})")
                        else:
                            print(f"   - {key}: {value}")
                    print("\n")

                yield {
                    "_index": self.index_name,
                    "_id": idx + 1,
                    "_source": source,
                }

        # Bulk ìƒ‰ì¸ ì‹¤í–‰
        success_count = 0
        error_count = 0

        for ok, result in helpers.streaming_bulk(
            self.client,
            generate_actions(),
            chunk_size=batch_size,
            raise_on_error=False,
        ):
            if ok:
                success_count += 1
            else:
                error_count += 1
                print(f"âš ï¸  ìƒ‰ì¸ ì˜¤ë¥˜: {result}")

            if (success_count + error_count) % 1000 == 0:
                print(f"   ìƒ‰ì¸ ì¤‘... ì„±ê³µ: {success_count:,}, ì‹¤íŒ¨: {error_count:,}")

        print("\nâœ… ë¬¸ì„œ ìƒ‰ì¸ ì™„ë£Œ!")
        print(f"   - ì„±ê³µ: {success_count:,}ê°œ")
        print(f"   - ì‹¤íŒ¨: {error_count:,}ê°œ")

        # ì¸ë±ìŠ¤ refresh
        self.client.indices.refresh(index=self.index_name)

    def get_index_stats(self):
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        stats = self.client.indices.stats(index=self.index_name)
        doc_count = stats["indices"][self.index_name]["total"]["docs"]["count"]
        size_bytes = stats["indices"][self.index_name]["total"]["store"][
            "size_in_bytes"
        ]

        print("\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„")
        print(f"   - ì¸ë±ìŠ¤: {self.index_name}")
        print(f"   - ë¬¸ì„œ ê°œìˆ˜: {doc_count:,}")
        print(f"   - ì €ì¥ í¬ê¸°: {size_bytes / (1024**2):.2f} MB")

    def show_sample_documents(self, count: int = 3):
        """ìƒ‰ì¸ëœ ë¬¸ì„œ ìƒ˜í”Œ ì¡°íšŒ"""
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ ìƒ‰ì¸ëœ ë¬¸ì„œ ìƒ˜í”Œ (ìµœëŒ€ {count}ê°œ)")
        print(f"{'='*80}\n")

        try:
            result = self.client.search(
                index=self.index_name, body={"query": {"match_all": {}}, "size": count}
            )

            hits = result["hits"]["hits"]

            if not hits:
                print("âŒ ìƒ‰ì¸ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            for i, hit in enumerate(hits, 1):
                doc = hit["_source"]
                print(f"[ë¬¸ì„œ {i}]")
                print(f"   ğŸ“„ title: {doc.get('title', 'N/A')}")
                print(f"   ğŸ“ namespace: {doc.get('namespace', 'N/A')}")
                print(f"   ğŸ“… timestamp: {doc.get('timestamp', 'N/A')}")
                if doc.get("text"):
                    print(f"   ğŸ“ text ê¸¸ì´: {len(doc.get('text', ''))} ì")
                    print(f"   ğŸ“ text ë¯¸ë¦¬ë³´ê¸°: {doc.get('text', '')[:200]}...")
                if doc.get("opening_text"):
                    print(f"   ğŸ“ opening_text: {doc.get('opening_text', '')[:150]}...")
                print(f"{'-'*80}\n")

        except (KeyError, ValueError) as e:
            print(f"âŒ ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    def search(self, query: str, size: int = 5):
        """
        ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ì–´
            size: ê²°ê³¼ ê°œìˆ˜
        """
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": ["title^3", "opening_text^2", "text"],
                    "type": "best_fields",
                }
            },
            "size": size,
            "_source": ["title", "opening_text", "timestamp", "namespace"],
        }

        result = self.client.search(index=self.index_name, body=search_body)
        return result

    def display_search_results(self, query: str, size: int = 5):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print(f"\n{'=' * 80}")
        print(f"ğŸ” ê²€ìƒ‰ì–´: '{query}'")
        print(f"{'=' * 80}")

        result = self.search(query, size)
        hits = result["hits"]["hits"]
        total = result["hits"]["total"]["value"]

        # ğŸ” ë””ë²„ê¹…: ê²€ìƒ‰ ì¿¼ë¦¬ì™€ ê²°ê³¼ ìƒì„¸ ì •ë³´
        print("\nğŸ” [ë””ë²„ê¹…] ê²€ìƒ‰ ì¿¼ë¦¬ ì •ë³´:")
        print("   - ê²€ìƒ‰ í•„ë“œ: title^3, opening_text^2, text")
        print("   - ë§¤ì¹­ ë°©ì‹: best_fields")
        print(f"   - ì´ ë§¤ì¹­ ë¬¸ì„œ ìˆ˜: {total:,}")
        print(f"   - ìš”ì²­í•œ ê²°ê³¼ ìˆ˜: {size}")
        print(f"   - ì‹¤ì œ ë°˜í™˜ëœ ê²°ê³¼ ìˆ˜: {len(hits)}")

        print(
            f"\nğŸ“Š ì´ {total:,}ê°œ ë¬¸ì„œ ì¤‘ ìƒìœ„ {min(size, len(hits))}ê°œ ê²°ê³¼ (BM25 Score ê¸°ì¤€)"
        )
        print(f"{'-' * 80}\n")

        if not hits:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("\nğŸ’¡ ê°€ëŠ¥í•œ ì›ì¸:")
            print("   1. ìƒ‰ì¸ëœ ë¬¸ì„œì— ê²€ìƒ‰ì–´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŒ")
            print("   2. Nori Tokenizerê°€ ê²€ìƒ‰ì–´ë¥¼ ë‹¤ë¥´ê²Œ ë¶„ì„í•¨")
            print("   3. namespace í•„í„° ë“±ìœ¼ë¡œ ì œì™¸ë¨")
            print("\nğŸ”§ ë””ë²„ê¹… ë°©ë²•:")
            print("   1. ìƒ‰ì¸ëœ ë¬¸ì„œ ìƒ˜í”Œ í™•ì¸")
            print("   2. Analyzer í…ŒìŠ¤íŠ¸ (Kibana Dev Tools):")
            print("      GET /kowiki_cirrus/_analyze")
            print("      {")
            print('        "analyzer": "korean_analyzer",')
            print(f'        "text": "{query}"')
            print("      }")
            return

        for rank, hit in enumerate(hits, 1):
            doc = hit["_source"]
            score = hit["_score"]

            print(f"[{rank}ìœ„] ğŸ“„ {doc.get('title', 'N/A')}")
            print(f"   ğŸ† BM25 Score: {score:.4f}")
            if "namespace" in doc:
                print(f"   ğŸ“ ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {doc['namespace']}")
            if "timestamp" in doc:
                print(f"   ğŸ“… ìµœì¢… ìˆ˜ì •: {doc['timestamp']}")
            if "opening_text" in doc:
                opening = doc["opening_text"]
                print(f"   ğŸ“ ìš”ì•½: {opening[:150]}...")
            print(f"{'-' * 80}\n")
